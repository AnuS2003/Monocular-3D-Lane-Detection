{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af55aede-d897-4961-8cf1-d115e682d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af1f956-6311-4b03-a57a-02f4eae0b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LaneAttentionBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = torch.sigmoid(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = x * attention\n",
    "        return x\n",
    "\n",
    "class LaneATT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneATT, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1)  # Output 1 channel for mask\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 -> ReLU -> Pooling\n",
    "        x = self.conv3(x)  # Last convolution layer outputting mask shape\n",
    "        return x  # Output shape should be [batch_size, 1, height, width]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ba66b8-2ae1-491e-873e-4c8f8c0a8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, frames_dir, masks_dir, transform=None):\n",
    "        self.frames_dir = frames_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.frames = sorted(os.listdir(frames_dir))\n",
    "        self.masks = sorted(os.listdir(masks_dir))\n",
    "        assert len(self.frames) == len(self.masks), \"Number of frames and masks should be the same\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = os.path.join(self.frames_dir, self.frames[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        frame = Image.open(frame_path).convert('RGB')  # Use PIL Image for frames\n",
    "        mask = Image.open(mask_path).convert('L')  # Use PIL Image for grayscale masks\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return frame, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb4043d-856d-4c9e-a1a0-348dd484700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a83c13d-0513-40b4-8b8f-ca8f8991992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 3626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set paths\n",
    "frames_dir = r'C:\\Users\\Stephen Fernandes\\Desktop\\ProjEct\\training\\frames'\n",
    "masks_dir = r'C:\\Users\\Stephen Fernandes\\Desktop\\ProjEct\\training\\lane-masks'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = LaneDataset(frames_dir, masks_dir, transform=transform)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Testing samples: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cd3d74-6438-4902-9418-eabe6ce08344",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneATT().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy with logits loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Check for mixed precision support\n",
    "use_amp = torch.cuda.is_available()\n",
    "\n",
    "if use_amp:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    scaler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0cb3a66-0ebb-4764-8205-d1cb0c3f4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen Fernandes\\AppData\\Local\\Temp\\ipykernel_14044\\1233849233.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Stephen Fernandes\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 5  # Adjust epochs as needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for frames, masks in train_loader:\n",
    "        frames, masks = frames.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_amp:  # Use mixed precision if available\n",
    "            with autocast():\n",
    "                outputs = model(frames)\n",
    "                outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)\n",
    "                loss = criterion(outputs, masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(frames)\n",
    "            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3eb84-cb2c-4b78-802b-921d6fd5d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (frames, masks) in enumerate(test_loader):\n",
    "            frames = frames.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(frames)\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "            frames = frames.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            masks = masks.cpu().squeeze(0).squeeze(0).numpy()\n",
    "            predictions = predictions.cpu().squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(frames)\n",
    "            plt.title(\"Original Frame\")\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(masks, cmap='gray')\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(predictions, cmap='gray')\n",
    "            plt.title(\"Model Prediction\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "            if i == 5:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96838435-61b9-4172-a969-91c5f410107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test dataset\n",
    "visualize_predictions(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed828d8a-6ae4-40fd-afe3-ea6807f82bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load test dataset (similar to training)\n",
    "# test_frames_dir = r'C:\\Users\\Stephen Fernandes\\Desktop\\ProjEct\\training\\frames'\n",
    "# test_masks_dir = r'C:\\Users\\Stephen Fernandes\\Desktop\\ProjEct\\training\\lane-masks'\n",
    "\n",
    "# test_dataset = LaneDataset(test_frames_dir, test_masks_dir, transform=transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Visualize predictions\n",
    "# # Visualize predictions on test data\n",
    "# visualize_predictions(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aad086-a0ce-4a18-948a-c2803c8178ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.profiler.profile(\n",
    "#     activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "#     record_shapes=True) as prof:\n",
    "#     for frames, masks in train_loader:\n",
    "#         outputs = model(frames)\n",
    "#         break\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
